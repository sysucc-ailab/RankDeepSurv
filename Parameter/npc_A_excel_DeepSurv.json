{"L2_reg": 2.8032470703125, "dropout": 0.1899951171875, "learning_rate": 0.023825980458515722, "lr_decay": 0.0005817382812499998, "momentum": 0.8383100585937501, "batch_norm": false, "activation": "selu", "standardize": true, "n_in": 8, "hidden_layers_sizes": [16, 16, 48,48]}
